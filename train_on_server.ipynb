{
 "cells": [
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "{\n",
    " \"cells\": [\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"# Fine-tuning Llama 3.1 8B on Compute Canada Server\\n\",\n",
    "    \"\\n\",\n",
    "    \"Server Specifications:\\n\",\n",
    "    \"- 40 CPU cores (Intel Xeon Gold 6148)\\n\",\n",
    "    \"- 187GB RAM\\n\",\n",
    "    \"- 4x Tesla V100-SXM2-16GB GPUs\\n\",\n",
    "    \"- CUDA 12.2\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 1. Environment Setup\\n\",\n",
    "    \"\\n\",\n",
    "    \"First, make sure you're running this notebook in a GPU node:\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Check GPU availability\\n\",\n",
    "    \"!nvidia-smi\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Install required packages\\n\",\n",
    "    \"!pip install -q transformers==4.49.0 datasets accelerate bitsandbytes trl peft\\n\",\n",
    "    \"!pip install -q einops wandb tyro diffusers hf_transfer\\n\",\n",
    "    \"\\n\",\n",
    "    \"# For better performance with Llama models\\n\",\n",
    "    \"!pip install -q sentencepiece protobuf safetensors\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"import os\\n\",\n",
    "    \"import torch\\n\",\n",
    "    \"from transformers import (\\n\",\n",
    "    \"    AutoModelForCausalLM,\\n\",\n",
    "    \"    AutoTokenizer,\\n\",\n",
    "    \"    BitsAndBytesConfig,\\n\",\n",
    "    \"    TrainingArguments,\\n\",\n",
    "    \")\\n\",\n",
    "    \"from peft import LoraConfig\\n\",\n",
    "    \"from trl import GRPOConfig, GRPOTrainer\\n\",\n",
    "    \"import json\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Set environment variables for better performance\\n\",\n",
    "    \"os.environ[\\\"TOKENIZERS_PARALLELISM\\\"] = \\\"true\\\"\\n\",\n",
    "    \"os.environ[\\\"CUDA_VISIBLE_DEVICES\\\"] = \\\"0,1,2,3\\\"  # Use all 4 GPUs\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 2. Load and Process Data\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def load_dataset(file_path):\\n\",\n",
    "    \"    with open(file_path, 'r', encoding='utf-8') as f:\\n\",\n",
    "    \"        data = json.load(f)\\n\",\n",
    "    \"    return data\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load your medical dataset\\n\",\n",
    "    \"dataset = load_dataset('data/datasets/merged_medical_data.json')\\n\",\n",
    "    \"print(f\\\"Loaded {len(dataset)} examples\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 3. Model Configuration\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Model configuration\\n\",\n",
    "    \"model_name = \\\"meta-llama/Meta-Llama-3.1-8B\\\"\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Quantization config for reduced memory usage\\n\",\n",
    "    \"bnb_config = BitsAndBytesConfig(\\n\",\n",
    "    \"    load_in_4bit=True,\\n\",\n",
    "    \"    bnb_4bit_quant_type=\\\"nf4\\\",\\n\",\n",
    "    \"    bnb_4bit_compute_dtype=torch.bfloat16,\\n\",\n",
    "    \"    bnb_4bit_use_double_quant=False,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# LoRA configuration for efficient fine-tuning\\n\",\n",
    "    \"lora_config = LoraConfig(\\n\",\n",
    "    \"    r=16,\\n\",\n",
    "    \"    lora_alpha=32,\\n\",\n",
    "    \"    target_modules=[\\\"q_proj\\\", \\\"k_proj\\\", \\\"v_proj\\\", \\\"o_proj\\\"],\\n\",\n",
    "    \"    lora_dropout=0.05,\\n\",\n",
    "    \"    bias=\\\"none\\\",\\n\",\n",
    "    \"    task_type=\\\"CAUSAL_LM\\\"\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 4. Load Model and Tokenizer\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Load tokenizer\\n\",\n",
    "    \"tokenizer = AutoTokenizer.from_pretrained(\\n\",\n",
    "    \"    model_name,\\n\",\n",
    "    \"    trust_remote_code=True,\\n\",\n",
    "    \"    padding_side=\\\"right\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"tokenizer.pad_token = tokenizer.eos_token\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Load model with quantization\\n\",\n",
    "    \"model = AutoModelForCausalLM.from_pretrained(\\n\",\n",
    "    \"    model_name,\\n\",\n",
    "    \"    quantization_config=bnb_config,\\n\",\n",
    "    \"    device_map=\\\"auto\\\",\\n\",\n",
    "    \"    trust_remote_code=True,\\n\",\n",
    "    \"    torch_dtype=torch.bfloat16,\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 5. Training Configuration\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Training arguments\\n\",\n",
    "    \"training_args = TrainingArguments(\\n\",\n",
    "    \"    output_dir=\\\"./outputs\\\",\\n\",\n",
    "    \"    num_train_epochs=3,\\n\",\n",
    "    \"    per_device_train_batch_size=4,\\n\",\n",
    "    \"    gradient_accumulation_steps=4,\\n\",\n",
    "    \"    learning_rate=2e-4,\\n\",\n",
    "    \"    bf16=True,\\n\",\n",
    "    \"    logging_steps=10,\\n\",\n",
    "    \"    save_steps=100,\\n\",\n",
    "    \"    eval_steps=100,\\n\",\n",
    "    \"    warmup_steps=100,\\n\",\n",
    "    \"    gradient_checkpointing=True,\\n\",\n",
    "    \"    gradient_checkpointing_kwargs={\\\"use_reentrant\\\": False},\\n\",\n",
    "    \"    group_by_length=True,\\n\",\n",
    "    \"    lr_scheduler_type=\\\"cosine\\\"\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# GRPO configuration\\n\",\n",
    "    \"grpo_config = GRPOConfig(\\n\",\n",
    "    \"    max_length=2048,\\n\",\n",
    "    \"    temperature=0.7,\\n\",\n",
    "    \"    top_k=50,\\n\",\n",
    "    \"    top_p=0.9,\\n\",\n",
    "    \"    pad_token_id=tokenizer.pad_token_id,\\n\",\n",
    "    \"    eos_token_id=tokenizer.eos_token_id,\\n\",\n",
    "    \")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 6. Training\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Initialize trainer\\n\",\n",
    "    \"trainer = GRPOTrainer(\\n\",\n",
    "    \"    model=model,\\n\",\n",
    "    \"    args=training_args,\\n\",\n",
    "    \"    train_dataset=dataset,\\n\",\n",
    "    \"    tokenizer=tokenizer,\\n\",\n",
    "    \"    grpo_config=grpo_config,\\n\",\n",
    "    \"    peft_config=lora_config,\\n\",\n",
    "    \")\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Start training\\n\",\n",
    "    \"trainer.train()\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 7. Save Model\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"# Save the final model\\n\",\n",
    "    \"trainer.save_model(\\\"./final_model\\\")\\n\",\n",
    "    \"tokenizer.save_pretrained(\\\"./final_model\\\")\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"markdown\",\n",
    "   \"metadata\": {},\n",
    "   \"source\": [\n",
    "    \"## 8. Inference\"\n",
    "   ]\n",
    "  },\n",
    "  {\n",
    "   \"cell_type\": \"code\",\n",
    "   \"execution_count\": null,\n",
    "   \"metadata\": {},\n",
    "   \"outputs\": [],\n",
    "   \"source\": [\n",
    "    \"def generate_response(prompt, max_length=512):\\n\",\n",
    "    \"    inputs = tokenizer(prompt, return_tensors=\\\"pt\\\").to(\\\"cuda\\\")\\n\",\n",
    "    \"    outputs = model.generate(\\n\",\n",
    "    \"        **inputs,\\n\",\n",
    "    \"        max_length=max_length,\\n\",\n",
    "    \"        temperature=0.7,\\n\",\n",
    "    \"        top_p=0.9,\\n\",\n",
    "    \"        do_sample=True,\\n\",\n",
    "    \"        pad_token_id=tokenizer.pad_token_id,\\n\",\n",
    "    \"    )\\n\",\n",
    "    \"    return tokenizer.decode(outputs[0], skip_special_tokens=True)\\n\",\n",
    "    \"\\n\",\n",
    "    \"# Test the model\\n\",\n",
    "    \"test_prompt = \\\"Patient presents with fever, cough, and shortness of breath. What is the most likely diagnosis?\\\"\\n\",\n",
    "    \"response = generate_response(test_prompt)\\n\",\n",
    "    \"print(response)\"\n",
    "   ]\n",
    "  }\n",
    " ],\n",
    " \"metadata\": {\n",
    "  \"kernelspec\": {\n",
    "   \"display_name\": \"Python 3\",\n",
    "   \"language\": \"python\",\n",
    "   \"name\": \"python3\"\n",
    "  },\n",
    "  \"language_info\": {\n",
    "   \"codemirror_mode\": {\n",
    "    \"name\": \"ipython\",\n",
    "    \"version\": 3\n",
    "   },\n",
    "   \"file_extension\": \".py\",\n",
    "   \"mimetype\": \"text/x-python\",\n",
    "   \"name\": \"python\",\n",
    "   \"nbconvert_exporter\": \"python\",\n",
    "   \"pygments_lexer\": \"ipython3\",\n",
    "   \"version\": \"3.12.9\"\n",
    "  }\n",
    " },\n",
    " \"nbformat\": 4,\n",
    " \"nbformat_minor\": 4\n",
    "}"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
